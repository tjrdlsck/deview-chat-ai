import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import logging
from pathlib import Path

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ì„¤ì • ---
BASE_MODEL_ID = "Qwen/Qwen3-8B"
ADAPTER_PATH = "./qwen3-8b-cs-interviewer-renew"
MERGED_MODEL_PATH = "./qwen3-8b-cs-interviewer-merged"

def merge_and_save():
    """
    LoRA ì–´ëŒ‘í„°ì™€ ê¸°ë³¸ ëª¨ë¸ì„ ë³‘í•©í•˜ê³ , ë¡œì»¬ì— ì—†ì„ ê²½ìš° ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
    """
    merged_model_path = Path(MERGED_MODEL_PATH)
    
    # --- ğŸ’¡ [í•µì‹¬ ì¶”ê°€] ì´ë¯¸ ë³‘í•©ëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸ ---
    # config.json íŒŒì¼ì˜ ì¡´ì¬ ì—¬ë¶€ë¡œ í™•ì¸
    if (merged_model_path / "config.json").exists():
        logger.info(f"ì´ë¯¸ ë³‘í•©ëœ ëª¨ë¸ì´ '{MERGED_MODEL_PATH}' ê²½ë¡œì— ì¡´ì¬í•©ë‹ˆë‹¤. ë³‘í•©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    logger.info(f"'{BASE_MODEL_ID}' ê¸°ë³¸ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤ (float16)...")
    logger.info("ë¡œì»¬ì— ëª¨ë¸ì´ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.")
    
    # ë³‘í•© ì‹œì—ëŠ” ì–‘ìí™” ì—†ì´ float16ìœ¼ë¡œ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
    )
    
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)

    logger.info(f"'{ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    model_to_merge = PeftModel.from_pretrained(base_model, ADAPTER_PATH)

    logger.info("ëª¨ë¸ê³¼ ì–´ëŒ‘í„°ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤...")
    # merge_and_unload()ëŠ” ë³‘í•© í›„ ì–´ëŒ‘í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ í•´ì œí•˜ì—¬ VRAMì„ ì ˆì•½í•©ë‹ˆë‹¤.
    merged_model = model_to_merge.merge_and_unload()
    logger.info("ë³‘í•© ì™„ë£Œ.")

    logger.info(f"ë³‘í•©ëœ ëª¨ë¸ì„ '{MERGED_MODEL_PATH}' ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤...")
    merged_model_path.mkdir(parents=True, exist_ok=True) # ì €ì¥ ê²½ë¡œ ìƒì„±
    merged_model.save_pretrained(MERGED_MODEL_PATH)
    tokenizer.save_pretrained(MERGED_MODEL_PATH)
    logger.info("ì €ì¥ ì™„ë£Œ.")

if __name__ == "__main__":
    try:
        merge_and_save()
    except Exception as e:
        logger.error(f"ë³‘í•© ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        logger.error("VRAMì´ ì¶©ë¶„í•œì§€, ì–´ëŒ‘í„° ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")